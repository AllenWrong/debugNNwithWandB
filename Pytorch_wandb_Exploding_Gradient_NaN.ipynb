{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Pytorch_wandb_Exploding_Gradient_NaN",
      "provenance": [],
      "authorship_tag": "ABX9TyMkb0ach3wpgd4C7jsbnMoM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ayulockin/debugNNwithWandB/blob/master/Pytorch_wandb_Exploding_Gradient_NaN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UJrTcams9S6G",
        "colab_type": "text"
      },
      "source": [
        "## Imports and Setups"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sVwYI9I3yGYp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 256
        },
        "outputId": "42ea8956-36f1-49cb-acba-db7bdbb68d2d"
      },
      "source": [
        "!pip install wandb -q"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 1.4MB 3.0MB/s \n",
            "\u001b[K     |████████████████████████████████| 102kB 11.7MB/s \n",
            "\u001b[K     |████████████████████████████████| 92kB 11.3MB/s \n",
            "\u001b[K     |████████████████████████████████| 460kB 58.7MB/s \n",
            "\u001b[K     |████████████████████████████████| 102kB 12.6MB/s \n",
            "\u001b[K     |████████████████████████████████| 71kB 8.9MB/s \n",
            "\u001b[K     |████████████████████████████████| 71kB 10.2MB/s \n",
            "\u001b[?25h  Building wheel for gql (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for subprocess32 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for shortuuid (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for watchdog (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for graphql-core (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ssj2mGK8yGVO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import wandb"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pBt2xCCvyGR4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "outputId": "3cdcf8b7-524c-46e2-ff21-4c39022efebf"
      },
      "source": [
        "!wandb login"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://app.wandb.ai/authorize\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter: 69f60a7711ce6b8bbae91ac6d15e45d6b1f1430e\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[32mSuccessfully logged in to Weights & Biases!\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rITPoqrguQnR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.datasets import make_regression\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "from torch.nn import functional as F\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TWaMxzG5zER9",
        "colab_type": "text"
      },
      "source": [
        "## Prepare Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XOywSx9CymQ7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# generate regression dataset\n",
        "X, y = make_regression(n_samples=1000, n_features=20, noise=0.1, random_state=1)\n",
        "# split into train and test\n",
        "n_train = 500\n",
        "x_train, x_test = X[:n_train, :], X[n_train:, :]\n",
        "y_train, y_test = y[:n_train], y[n_train:]\n",
        "\n",
        "y_train = y_train.reshape((y_train.shape+(1,)))\n",
        "y_test = y_train.reshape((y_test.shape+(1,)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q8hqhyqazNrI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train, y_train = torch.Tensor(x_train), torch.Tensor(y_train)\n",
        "x_test, y_test = torch.Tensor(x_test), torch.Tensor(y_test)\n",
        "\n",
        "trainloader = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(x_train, y_train),\n",
        "                                          batch_size=32, shuffle=True)\n",
        "testloader = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(x_test, y_test),\n",
        "                                         batch_size=32, shuffle=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "95nqN1bj1SEC",
        "colab_type": "text"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sMBvF0Aj1T-A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.fc1 = nn.Linear(20, 25, bias=False)\n",
        "        self.dropout = nn.Dropout(0.7)\n",
        "        self.fc2 = nn.Linear(25, 1, bias=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = F.relu(x) ## Notice\n",
        "        x = self.fc2(x)\n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z3Cv0XLX54Xo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(model, train_loader, optimizer, criterion, epoch, steps_per_epoch=20):\n",
        "  # Switch model to training mode. This is necessary for layers like dropout, batchnorm etc which behave differently in training and evaluation mode\n",
        "  model.train()\n",
        "  train_total = 0\n",
        "  train_correct = 0\n",
        "\n",
        "  # We loop over the data iterator, and feed the inputs to the network and adjust the weights.\n",
        "  for batch_idx, (data, target) in enumerate(train_loader, start=0):\n",
        "    if batch_idx > steps_per_epoch:\n",
        "      break\n",
        "    # Load the input features and labels from the training dataset\n",
        "    data, target = data, target\n",
        "    \n",
        "    # Reset the gradients to 0 for all learnable weight parameters\n",
        "    optimizer.zero_grad()\n",
        "    \n",
        "    # Forward pass: Pass image data from training dataset, make predictions about class image belongs to (0-9 in this case)\n",
        "    output = model(data)\n",
        "    \n",
        "    # Define our loss function, and compute the loss\n",
        "    loss = criterion(output, target)\n",
        "            \n",
        "    # Backward pass: compute the gradients of the loss w.r.t. the model's parameters\n",
        "    loss.backward()\n",
        "\n",
        "    # Update the neural network weights\n",
        "    optimizer.step()\n",
        "\n",
        "  print('Epoch [{}], Loss: {} '.format(epoch, loss.item()), end='')\n",
        "  wandb.log({'Train Loss': loss.item()})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vHcCrrPJ65p-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test(model, test_loader, criterion):\n",
        "  # Switch model to evaluation mode. This is necessary for layers like dropout, batchnorm etc which behave differently in training and evaluation mode\n",
        "  model.eval()\n",
        "  \n",
        "  test_loss = 0\n",
        "  test_total = 0\n",
        "\n",
        "  with torch.no_grad():\n",
        "      for data, target in test_loader:\n",
        "          # Load the input features and labels from the test dataset\n",
        "          data, target = data, target\n",
        "          \n",
        "          # Make predictions: Pass image data from test dataset, make predictions about class image belongs to (0-9 in this case)\n",
        "          output = model(data)\n",
        "          \n",
        "          # Compute the loss sum up batch loss\n",
        "          test_loss += criterion(output, target).item()\n",
        "          test_total += target.size(0)\n",
        "          \n",
        "  print(' Test_loss: {}'.format(test_loss/test_total))\n",
        "  wandb.log({'Test Loss': test_loss/test_total})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "7d18e6e6-5cc8-4319-9f78-12e53d8c1f8b",
        "id": "AlOh85oS7Fm_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        }
      },
      "source": [
        "net = Net()\n",
        "print(net)\n",
        "\n",
        "criterion = torch.nn.MSELoss()\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.01, momentum=0.9)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Net(\n",
            "  (fc1): Linear(in_features=20, out_features=25, bias=False)\n",
            "  (dropout): Dropout(p=0.7, inplace=False)\n",
            "  (fc2): Linear(in_features=25, out_features=1, bias=False)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "91b250d0-89ef-4d65-a5e9-f00364253698",
        "id": "VJBjNz-17Fmh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "wandb.init(project='explodingdebug')\n",
        "wandb.watch(net, log='all')\n",
        "\n",
        "for epoch in range(100):\n",
        "  train(net, trainloader, optimizer, criterion, epoch)\n",
        "  test(net, testloader, criterion)\n",
        "\n",
        "print('Finished Training')"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
              "                Project page: <a href=\"https://app.wandb.ai/ayush-thakur/explodingdebug\" target=\"_blank\">https://app.wandb.ai/ayush-thakur/explodingdebug</a><br/>\n",
              "                Run page: <a href=\"https://app.wandb.ai/ayush-thakur/explodingdebug/runs/96dm2v6j\" target=\"_blank\">https://app.wandb.ai/ayush-thakur/explodingdebug/runs/96dm2v6j</a><br/>\n",
              "            "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [0], Loss: nan  Test_loss: nan\n",
            "Epoch [1], Loss: nan  Test_loss: nan\n",
            "Epoch [2], Loss: nan  Test_loss: nan\n",
            "Epoch [3], Loss: nan  Test_loss: nan\n",
            "Epoch [4], Loss: nan  Test_loss: nan\n",
            "Epoch [5], Loss: nan  Test_loss: nan\n",
            "Epoch [6], Loss: nan  Test_loss: nan\n",
            "Epoch [7], Loss: nan  Test_loss: nan\n",
            "Epoch [8], Loss: nan  Test_loss: nan\n",
            "Epoch [9], Loss: nan  Test_loss: nan\n",
            "Epoch [10], Loss: nan  Test_loss: nan\n",
            "Epoch [11], Loss: nan  Test_loss: nan\n",
            "Epoch [12], Loss: nan  Test_loss: nan\n",
            "Epoch [13], Loss: nan  Test_loss: nan\n",
            "Epoch [14], Loss: nan  Test_loss: nan\n",
            "Epoch [15], Loss: nan  Test_loss: nan\n",
            "Epoch [16], Loss: nan  Test_loss: nan\n",
            "Epoch [17], Loss: nan  Test_loss: nan\n",
            "Epoch [18], Loss: nan  Test_loss: nan\n",
            "Epoch [19], Loss: nan  Test_loss: nan\n",
            "Epoch [20], Loss: nan  Test_loss: nan\n",
            "Epoch [21], Loss: nan  Test_loss: nan\n",
            "Epoch [22], Loss: nan  Test_loss: nan\n",
            "Epoch [23], Loss: nan  Test_loss: nan\n",
            "Epoch [24], Loss: nan  Test_loss: nan\n",
            "Epoch [25], Loss: nan  Test_loss: nan\n",
            "Epoch [26], Loss: nan  Test_loss: nan\n",
            "Epoch [27], Loss: nan  Test_loss: nan\n",
            "Epoch [28], Loss: nan  Test_loss: nan\n",
            "Epoch [29], Loss: nan  Test_loss: nan\n",
            "Epoch [30], Loss: nan  Test_loss: nan\n",
            "Epoch [31], Loss: nan  Test_loss: nan\n",
            "Epoch [32], Loss: nan  Test_loss: nan\n",
            "Epoch [33], Loss: nan  Test_loss: nan\n",
            "Epoch [34], Loss: nan  Test_loss: nan\n",
            "Epoch [35], Loss: nan  Test_loss: nan\n",
            "Epoch [36], Loss: nan  Test_loss: nan\n",
            "Epoch [37], Loss: nan  Test_loss: nan\n",
            "Epoch [38], Loss: nan  Test_loss: nan\n",
            "Epoch [39], Loss: nan  Test_loss: nan\n",
            "Epoch [40], Loss: nan  Test_loss: nan\n",
            "Epoch [41], Loss: nan  Test_loss: nan\n",
            "Epoch [42], Loss: nan  Test_loss: nan\n",
            "Epoch [43], Loss: nan  Test_loss: nan\n",
            "Epoch [44], Loss: nan  Test_loss: nan\n",
            "Epoch [45], Loss: nan  Test_loss: nan\n",
            "Epoch [46], Loss: nan  Test_loss: nan\n",
            "Epoch [47], Loss: nan  Test_loss: nan\n",
            "Epoch [48], Loss: nan  Test_loss: nan\n",
            "Epoch [49], Loss: nan  Test_loss: nan\n",
            "Epoch [50], Loss: nan  Test_loss: nan\n",
            "Epoch [51], Loss: nan  Test_loss: nan\n",
            "Epoch [52], Loss: nan  Test_loss: nan\n",
            "Epoch [53], Loss: nan  Test_loss: nan\n",
            "Epoch [54], Loss: nan  Test_loss: nan\n",
            "Epoch [55], Loss: nan  Test_loss: nan\n",
            "Epoch [56], Loss: nan  Test_loss: nan\n",
            "Epoch [57], Loss: nan  Test_loss: nan\n",
            "Epoch [58], Loss: nan  Test_loss: nan\n",
            "Epoch [59], Loss: nan  Test_loss: nan\n",
            "Epoch [60], Loss: nan  Test_loss: nan\n",
            "Epoch [61], Loss: nan  Test_loss: nan\n",
            "Epoch [62], Loss: nan  Test_loss: nan\n",
            "Epoch [63], Loss: nan  Test_loss: nan\n",
            "Epoch [64], Loss: nan  Test_loss: nan\n",
            "Epoch [65], Loss: nan  Test_loss: nan\n",
            "Epoch [66], Loss: nan  Test_loss: nan\n",
            "Epoch [67], Loss: nan  Test_loss: nan\n",
            "Epoch [68], Loss: nan  Test_loss: nan\n",
            "Epoch [69], Loss: nan  Test_loss: nan\n",
            "Epoch [70], Loss: nan  Test_loss: nan\n",
            "Epoch [71], Loss: nan  Test_loss: nan\n",
            "Epoch [72], Loss: nan  Test_loss: nan\n",
            "Epoch [73], Loss: nan  Test_loss: nan\n",
            "Epoch [74], Loss: nan  Test_loss: nan\n",
            "Epoch [75], Loss: nan  Test_loss: nan\n",
            "Epoch [76], Loss: nan  Test_loss: nan\n",
            "Epoch [77], Loss: nan  Test_loss: nan\n",
            "Epoch [78], Loss: nan  Test_loss: nan\n",
            "Epoch [79], Loss: nan  Test_loss: nan\n",
            "Epoch [80], Loss: nan  Test_loss: nan\n",
            "Epoch [81], Loss: nan  Test_loss: nan\n",
            "Epoch [82], Loss: nan  Test_loss: nan\n",
            "Epoch [83], Loss: nan  Test_loss: nan\n",
            "Epoch [84], Loss: nan  Test_loss: nan\n",
            "Epoch [85], Loss: nan  Test_loss: nan\n",
            "Epoch [86], Loss: nan  Test_loss: nan\n",
            "Epoch [87], Loss: nan  Test_loss: nan\n",
            "Epoch [88], Loss: nan  Test_loss: nan\n",
            "Epoch [89], Loss: nan  Test_loss: nan\n",
            "Epoch [90], Loss: nan  Test_loss: nan\n",
            "Epoch [91], Loss: nan  Test_loss: nan\n",
            "Epoch [92], Loss: nan  Test_loss: nan\n",
            "Epoch [93], Loss: nan  Test_loss: nan\n",
            "Epoch [94], Loss: nan  Test_loss: nan\n",
            "Epoch [95], Loss: nan  Test_loss: nan\n",
            "Epoch [96], Loss: nan  Test_loss: nan\n",
            "Epoch [97], Loss: nan  Test_loss: nan\n",
            "Epoch [98], Loss: nan  Test_loss: nan\n",
            "Epoch [99], Loss: nan  Test_loss: nan\n",
            "Finished Training\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eTnBHxVTzNkd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def trainModified(model, train_loader, optimizer, criterion, epoch, steps_per_epoch=20):\n",
        "  # Switch model to training mode. This is necessary for layers like dropout, batchnorm etc which behave differently in training and evaluation mode\n",
        "  model.train()\n",
        "  train_total = 0\n",
        "  train_correct = 0\n",
        "\n",
        "  # We loop over the data iterator, and feed the inputs to the network and adjust the weights.\n",
        "  for batch_idx, (data, target) in enumerate(train_loader, start=0):\n",
        "    if batch_idx > steps_per_epoch:\n",
        "      break\n",
        "    # Load the input features and labels from the training dataset\n",
        "    data, target = data, target\n",
        "    \n",
        "    # Reset the gradients to 0 for all learnable weight parameters\n",
        "    optimizer.zero_grad()\n",
        "    \n",
        "    # Forward pass: Pass image data from training dataset, make predictions about class image belongs to (0-9 in this case)\n",
        "    output = model(data)\n",
        "    \n",
        "    # Define our loss function, and compute the loss\n",
        "    loss = criterion(output, target)\n",
        "\n",
        "    # Backward pass: compute the gradients of the loss w.r.t. the model's parameters\n",
        "    loss.backward()\n",
        "\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), 4.0)\n",
        "    # Update the neural network weights\n",
        "    optimizer.step()\n",
        "\n",
        "  print('Epoch [{}], Loss: {} '.format(epoch, loss.item()), end='')\n",
        "  wandb.log({'Train Loss': loss.item()})\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "1c27d221-2aaf-444b-bfe7-0ca193b8c6e8",
        "id": "Y1KkK9hA7H88",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        }
      },
      "source": [
        "del net\n",
        "net = Net()\n",
        "print(net)\n",
        "\n",
        "criterion = torch.nn.MSELoss()\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.01, momentum=0.9, dampening=1e-6)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Net(\n",
            "  (fc1): Linear(in_features=20, out_features=25, bias=False)\n",
            "  (dropout): Dropout(p=0.7, inplace=False)\n",
            "  (fc2): Linear(in_features=25, out_features=1, bias=False)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "9e9b98eb-d9b9-48bb-e194-9c3a2f3e7f3f",
        "id": "lRji-GnK7H9G",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "wandb.init(project='explodingdebug')\n",
        "wandb.watch(net, log='all')\n",
        "\n",
        "for epoch in range(100):\n",
        "  trainModified(net, trainloader, optimizer, criterion, epoch)\n",
        "  test(net, testloader, criterion)\n",
        "\n",
        "print('Finished Training')"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
              "                Project page: <a href=\"https://app.wandb.ai/ayush-thakur/explodingdebug\" target=\"_blank\">https://app.wandb.ai/ayush-thakur/explodingdebug</a><br/>\n",
              "                Run page: <a href=\"https://app.wandb.ai/ayush-thakur/explodingdebug/runs/9c4bi503\" target=\"_blank\">https://app.wandb.ai/ayush-thakur/explodingdebug/runs/9c4bi503</a><br/>\n",
              "            "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [0], Loss: 24005.736328125  Test_loss: 648.1969921875\n",
            "Epoch [1], Loss: 13096.1015625  Test_loss: 656.34944921875\n",
            "Epoch [2], Loss: 11086.3447265625  Test_loss: 667.07905078125\n",
            "Epoch [3], Loss: 6045.63427734375  Test_loss: 758.681412109375\n",
            "Epoch [4], Loss: 1780.2254638671875  Test_loss: 1005.92348828125\n",
            "Epoch [5], Loss: 427.66156005859375  Test_loss: 1372.0641875\n",
            "Epoch [6], Loss: 26.74477195739746  Test_loss: 1371.44294140625\n",
            "Epoch [7], Loss: 23.26637840270996  Test_loss: 1382.2956015625\n",
            "Epoch [8], Loss: 11.095369338989258  Test_loss: 1403.86296484375\n",
            "Epoch [9], Loss: 5.545734405517578  Test_loss: 1408.36526171875\n",
            "Epoch [10], Loss: 2.4922313690185547  Test_loss: 1388.17856640625\n",
            "Epoch [11], Loss: 4.9311065673828125  Test_loss: 1390.03403515625\n",
            "Epoch [12], Loss: 1.9409598112106323  Test_loss: 1390.3574296875\n",
            "Epoch [13], Loss: 1.8449726104736328  Test_loss: 1392.67828125\n",
            "Epoch [14], Loss: 2.713657855987549  Test_loss: 1378.0353125\n",
            "Epoch [15], Loss: 2.134899616241455  Test_loss: 1395.82501953125\n",
            "Epoch [16], Loss: 1.685463309288025  Test_loss: 1385.867859375\n",
            "Epoch [17], Loss: 4.056157112121582  Test_loss: 1375.65508984375\n",
            "Epoch [18], Loss: 2.06233286857605  Test_loss: 1394.03038671875\n",
            "Epoch [19], Loss: 2.032496452331543  Test_loss: 1379.255109375\n",
            "Epoch [20], Loss: 3.3886630535125732  Test_loss: 1387.8764140625\n",
            "Epoch [21], Loss: 0.968360185623169  Test_loss: 1386.87884375\n",
            "Epoch [22], Loss: 4.144728660583496  Test_loss: 1390.8984375\n",
            "Epoch [23], Loss: 0.9546118378639221  Test_loss: 1377.8138984375\n",
            "Epoch [24], Loss: 1.7927756309509277  Test_loss: 1384.66169921875\n",
            "Epoch [25], Loss: 3.3626644611358643  Test_loss: 1376.30678125\n",
            "Epoch [26], Loss: 2.7833540439605713  Test_loss: 1393.7263359375\n",
            "Epoch [27], Loss: 1.2874304056167603  Test_loss: 1382.04279296875\n",
            "Epoch [28], Loss: 1.4630351066589355  Test_loss: 1381.86566015625\n",
            "Epoch [29], Loss: 1.7165369987487793  Test_loss: 1407.247078125\n",
            "Epoch [30], Loss: 3.35203218460083  Test_loss: 1397.73315234375\n",
            "Epoch [31], Loss: 1.7187644243240356  Test_loss: 1390.86003515625\n",
            "Epoch [32], Loss: 1.746169924736023  Test_loss: 1380.2593984375\n",
            "Epoch [33], Loss: 1.0302671194076538  Test_loss: 1405.96415234375\n",
            "Epoch [34], Loss: 2.1672863960266113  Test_loss: 1387.85184375\n",
            "Epoch [35], Loss: 3.922538995742798  Test_loss: 1394.66174609375\n",
            "Epoch [36], Loss: 3.3561835289001465  Test_loss: 1393.45916015625\n",
            "Epoch [37], Loss: 1.6595712900161743  Test_loss: 1377.312546875\n",
            "Epoch [38], Loss: 1.501872181892395  Test_loss: 1388.87730078125\n",
            "Epoch [39], Loss: 1.9380687475204468  Test_loss: 1396.3904921875\n",
            "Epoch [40], Loss: 3.5222744941711426  Test_loss: 1395.08399609375\n",
            "Epoch [41], Loss: 1.8865711688995361  Test_loss: 1390.49176953125\n",
            "Epoch [42], Loss: 2.17060923576355  Test_loss: 1375.330171875\n",
            "Epoch [43], Loss: 2.5257787704467773  Test_loss: 1389.538125\n",
            "Epoch [44], Loss: 1.391767978668213  Test_loss: 1398.29688671875\n",
            "Epoch [45], Loss: 2.878511905670166  Test_loss: 1378.93461328125\n",
            "Epoch [46], Loss: 4.372735500335693  Test_loss: 1401.59180078125\n",
            "Epoch [47], Loss: 4.388952255249023  Test_loss: 1393.542625\n",
            "Epoch [48], Loss: 4.077722549438477  Test_loss: 1377.35067578125\n",
            "Epoch [49], Loss: 2.2461206912994385  Test_loss: 1390.5691171875\n",
            "Epoch [50], Loss: 1.398695468902588  Test_loss: 1393.3330703125\n",
            "Epoch [51], Loss: 2.6865150928497314  Test_loss: 1401.37178515625\n",
            "Epoch [52], Loss: 1.9160808324813843  Test_loss: 1397.77273828125\n",
            "Epoch [53], Loss: 1.6206518411636353  Test_loss: 1384.434359375\n",
            "Epoch [54], Loss: 2.0201172828674316  Test_loss: 1391.643203125\n",
            "Epoch [55], Loss: 3.154313802719116  Test_loss: 1380.676734375\n",
            "Epoch [56], Loss: 2.3107099533081055  Test_loss: 1381.6509453125\n",
            "Epoch [57], Loss: 1.5522489547729492  Test_loss: 1390.72341015625\n",
            "Epoch [58], Loss: 1.4710291624069214  Test_loss: 1391.6556640625\n",
            "Epoch [59], Loss: 2.8894948959350586  Test_loss: 1380.71484375\n",
            "Epoch [60], Loss: 2.0387747287750244  Test_loss: 1381.3110078125\n",
            "Epoch [61], Loss: 4.076231956481934  Test_loss: 1400.1674296875\n",
            "Epoch [62], Loss: 2.6303908824920654  Test_loss: 1412.2176015625\n",
            "Epoch [63], Loss: 1.42528235912323  Test_loss: 1391.06019140625\n",
            "Epoch [64], Loss: 5.597474098205566  Test_loss: 1396.472921875\n",
            "Epoch [65], Loss: 5.153275012969971  Test_loss: 1383.4370390625\n",
            "Epoch [66], Loss: 3.4963808059692383  Test_loss: 1375.66708984375\n",
            "Epoch [67], Loss: 1.6664786338806152  Test_loss: 1381.65990234375\n",
            "Epoch [68], Loss: 3.6568851470947266  Test_loss: 1398.47744140625\n",
            "Epoch [69], Loss: 3.23715877532959  Test_loss: 1377.13799609375\n",
            "Epoch [70], Loss: 2.3039095401763916  Test_loss: 1378.16812890625\n",
            "Epoch [71], Loss: 3.663531541824341  Test_loss: 1385.7954921875\n",
            "Epoch [72], Loss: 1.4028137922286987  Test_loss: 1400.20973046875\n",
            "Epoch [73], Loss: 1.9920752048492432  Test_loss: 1380.18894140625\n",
            "Epoch [74], Loss: 4.741968631744385  Test_loss: 1394.818953125\n",
            "Epoch [75], Loss: 1.2727622985839844  Test_loss: 1407.69931640625\n",
            "Epoch [76], Loss: 0.8372445106506348  Test_loss: 1390.36184375\n",
            "Epoch [77], Loss: 2.9430110454559326  Test_loss: 1389.054234375\n",
            "Epoch [78], Loss: 0.9508330225944519  Test_loss: 1393.36406640625\n",
            "Epoch [79], Loss: 1.4087597131729126  Test_loss: 1394.17091015625\n",
            "Epoch [80], Loss: 2.1409642696380615  Test_loss: 1401.6779921875\n",
            "Epoch [81], Loss: 2.217461109161377  Test_loss: 1372.2902890625\n",
            "Epoch [82], Loss: 2.0496554374694824  Test_loss: 1389.6626015625\n",
            "Epoch [83], Loss: 2.904942512512207  Test_loss: 1390.1698828125\n",
            "Epoch [84], Loss: 1.1139042377471924  Test_loss: 1400.2694140625\n",
            "Epoch [85], Loss: 2.655024766921997  Test_loss: 1394.7992578125\n",
            "Epoch [86], Loss: 2.4425370693206787  Test_loss: 1379.815609375\n",
            "Epoch [87], Loss: 1.6432231664657593  Test_loss: 1389.39587890625\n",
            "Epoch [88], Loss: 3.0271494388580322  Test_loss: 1398.9845234375\n",
            "Epoch [89], Loss: 1.4205949306488037  Test_loss: 1393.840546875\n",
            "Epoch [90], Loss: 1.6596988439559937  Test_loss: 1378.28283203125\n",
            "Epoch [91], Loss: 1.437886118888855  Test_loss: 1389.88959375\n",
            "Epoch [92], Loss: 0.8348333239555359  Test_loss: 1394.71749609375\n",
            "Epoch [93], Loss: 3.3948922157287598  Test_loss: 1384.9400390625\n",
            "Epoch [94], Loss: 1.3029372692108154  Test_loss: 1383.43142578125\n",
            "Epoch [95], Loss: 0.6367129683494568  Test_loss: 1392.40187890625\n",
            "Epoch [96], Loss: 1.9750475883483887  Test_loss: 1382.82291796875\n",
            "Epoch [97], Loss: 1.3319839239120483  Test_loss: 1378.1720859375\n",
            "Epoch [98], Loss: 3.20426607131958  Test_loss: 1394.9129609375\n",
            "Epoch [99], Loss: 0.9687417149543762  Test_loss: 1393.18753125\n",
            "Finished Training\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qQzYwMxwZfmZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}