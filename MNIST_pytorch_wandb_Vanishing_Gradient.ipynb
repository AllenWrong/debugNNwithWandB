{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MNIST_pytorch_wandb_Vanishing_Gradient",
      "provenance": [],
      "authorship_tag": "ABX9TyPiDiAw1GmeqQA9EKJEiG4N",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ayulockin/debugNNwithWandB/blob/master/MNIST_pytorch_wandb_Vanishing_Gradient.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UJrTcams9S6G",
        "colab_type": "text"
      },
      "source": [
        "## Imports and Setups"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HESv1l--3AAa",
        "colab_type": "code",
        "outputId": "61e3086f-e642-4b44-8c44-6287fcbe111b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 256
        }
      },
      "source": [
        "!pip install wandb -q"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 1.4MB 5.0MB/s \n",
            "\u001b[K     |████████████████████████████████| 102kB 14.4MB/s \n",
            "\u001b[K     |████████████████████████████████| 102kB 14.9MB/s \n",
            "\u001b[K     |████████████████████████████████| 92kB 13.6MB/s \n",
            "\u001b[K     |████████████████████████████████| 460kB 61.4MB/s \n",
            "\u001b[K     |████████████████████████████████| 71kB 11.9MB/s \n",
            "\u001b[K     |████████████████████████████████| 71kB 9.0MB/s \n",
            "\u001b[?25h  Building wheel for gql (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for subprocess32 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for watchdog (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for shortuuid (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for graphql-core (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vg0iv_RPT3YG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import wandb"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4zcjcQ3-UH2H",
        "colab_type": "code",
        "outputId": "7201baf1-75b3-4ccd-c466-8ef4daef186a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "source": [
        "!wandb login"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://app.wandb.ai/authorize\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter: 69f60a7711ce6b8bbae91ac6d15e45d6b1f1430e\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[32mSuccessfully logged in to Weights & Biases!\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_RFZ81yY3Rfl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "from torch.nn import functional as F\n",
        "import torchvision\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DtakKUgw9Ype",
        "colab_type": "text"
      },
      "source": [
        "#### For GPU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NiEHlMTh4ecC",
        "colab_type": "code",
        "outputId": "d31228ee-10b2-4ab4-d50e-0bb08403c551",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TIxBcFVg9clX",
        "colab_type": "text"
      },
      "source": [
        "## MNIST Hand written Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FtM4FMPP3imH",
        "colab_type": "code",
        "outputId": "2f3fa6f0-933a-402e-c9ba-08821b326296",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 294
        }
      },
      "source": [
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize((0.1307,), (0.3081,))])\n",
        "\n",
        "trainset = torchvision.datasets.MNIST(root='./data', train=True,\n",
        "                                        download=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
        "\n",
        "testset = torchvision.datasets.MNIST(root='./data', train=False,\n",
        "                                       download=True, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=False)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "9920512it [00:01, 9253332.02it/s]                            \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 0/28881 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "32768it [00:00, 141591.26it/s]           \n",
            "  0%|          | 0/1648877 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "1654784it [00:00, 2271908.60it/s]                            \n",
            "0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "8192it [00:00, 52847.07it/s]            "
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "Processing...\n",
            "Done!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "33T4NzgLjNlT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "classes = ('0', '1', '2', '3', '4', '5', '6', '7', '8', '9')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_8luRi8q9pyh",
        "colab_type": "text"
      },
      "source": [
        "## Models\n",
        "\n",
        "1.   `NetwithIssue`: This model uses `sigmoid` as activation function in each layer. Earlier when `relu` was not discovered `sigmoid` created the problem of **Vanishing Gradient**. Due to this deep networks were not possible and it took longer training time to converge. \n",
        "2.   `Net`: This model uses `relu` as activation function in each layer. This activation solves the problem of vanishing gradient.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CUOU5phlFoZn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class NetwithIssue(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(NetwithIssue, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 32, 3, 1, bias=False)\n",
        "        self.conv2 = nn.Conv2d(32, 64, 3, 1, bias=False)\n",
        "\n",
        "        self.fc1 = nn.Linear(9216, 128, bias=False)\n",
        "        self.fc2 = nn.Linear(128, 10, bias=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        ## Conv 1st Block\n",
        "        x = self.conv1(x)\n",
        "        x = torch.sigmoid(x) ## Notice\n",
        "        x = self.conv2(x)\n",
        "        x = torch.sigmoid(x) ## Notice\n",
        "        x = F.max_pool2d(x, 2)\n",
        "\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.fc1(x)\n",
        "        x = torch.sigmoid(x) ## Notice\n",
        "        x = self.fc2(x)\n",
        "        output = F.log_softmax(x, dim=1)\n",
        "        return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z3rki8u__TU4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 32, 3, 1, bias=False)\n",
        "        self.conv2 = nn.Conv2d(32, 64, 3, 1, bias=False)\n",
        "\n",
        "        self.fc1 = nn.Linear(9216, 128, bias=False)\n",
        "        self.fc2 = nn.Linear(128, 10, bias=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        ## Conv 1st Block\n",
        "        x = self.conv1(x)\n",
        "        x = F.relu(x) ## Notice\n",
        "        x = self.conv2(x)\n",
        "        x = F.relu(x) ## Notice\n",
        "        x = F.max_pool2d(x, 2)\n",
        "\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.fc1(x)\n",
        "        x = F.relu(x) ## Notice\n",
        "        x = self.fc2(x)\n",
        "        output = F.log_softmax(x, dim=1)\n",
        "        return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z3Cv0XLX54Xo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(model, device, train_loader, optimizer, epoch, steps_per_epoch=20):\n",
        "  # Switch model to training mode. This is necessary for layers like dropout, batchnorm etc which behave differently in training and evaluation mode\n",
        "  model.train()\n",
        "  train_total = 0\n",
        "  train_correct = 0\n",
        "\n",
        "  # We loop over the data iterator, and feed the inputs to the network and adjust the weights.\n",
        "  for batch_idx, (data, target) in enumerate(train_loader, start=0):\n",
        "    if batch_idx > steps_per_epoch:\n",
        "      break\n",
        "    # Load the input features and labels from the training dataset\n",
        "    data, target = data.to(device), target.to(device)\n",
        "    \n",
        "    # Reset the gradients to 0 for all learnable weight parameters\n",
        "    optimizer.zero_grad()\n",
        "    \n",
        "    # Forward pass: Pass image data from training dataset, make predictions about class image belongs to (0-9 in this case)\n",
        "    output = model(data)\n",
        "    \n",
        "    # Define our loss function, and compute the loss\n",
        "    loss = F.nll_loss(output, target)\n",
        "\n",
        "    scores, predictions = torch.max(output.data, 1)\n",
        "    train_total += target.size(0)\n",
        "    train_correct += int(sum(predictions == target))\n",
        "            \n",
        "    # Backward pass: compute the gradients of the loss w.r.t. the model's parameters\n",
        "    loss.backward()\n",
        "    \n",
        "    # Update the neural network weights\n",
        "    optimizer.step()\n",
        "\n",
        "  acc = round((train_correct / train_total) * 100, 2)\n",
        "  print('Epoch [{}], Loss: {}, Accuracy: {}, '.format(epoch, loss.item(), acc), end='')\n",
        "  wandb.log({'Train Loss': loss.item(), 'Train Accuracy': acc})\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vHcCrrPJ65p-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test(model, device, test_loader, classes):\n",
        "  # Switch model to evaluation mode. This is necessary for layers like dropout, batchnorm etc which behave differently in training and evaluation mode\n",
        "  model.eval()\n",
        "  \n",
        "  test_loss = 0\n",
        "  test_total = 0\n",
        "  test_correct = 0\n",
        "\n",
        "  with torch.no_grad():\n",
        "      for data, target in test_loader:\n",
        "          # Load the input features and labels from the test dataset\n",
        "          data, target = data.to(device), target.to(device)\n",
        "          \n",
        "          # Make predictions: Pass image data from test dataset, make predictions about class image belongs to (0-9 in this case)\n",
        "          output = model(data)\n",
        "          \n",
        "          # Compute the loss sum up batch loss\n",
        "          test_loss += F.nll_loss(output, target, reduction='sum').item()\n",
        "          \n",
        "          scores, predictions = torch.max(output.data, 1)\n",
        "          test_total += target.size(0)\n",
        "          test_correct += int(sum(predictions == target))\n",
        "          \n",
        "  acc = round((test_correct / test_total) * 100, 2)\n",
        "  print(' Test_loss: {}, Test_accuracy: {}'.format(test_loss/test_total, acc))\n",
        "  wandb.log({'Test Loss': test_loss/test_total, 'Test Accuracy': acc})\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lZvNwsrk_xFa",
        "colab_type": "text"
      },
      "source": [
        "## Vanishing Gradient (Network with Issue)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KYSE0Psw38w0",
        "colab_type": "code",
        "outputId": "8c7e616e-0ea7-4024-dc7d-1882d3f18848",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        }
      },
      "source": [
        "net = NetwithIssue().to(device)\n",
        "print(net)\n",
        "\n",
        "optimizer = optim.Adam(net.parameters())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "NetwithIssue(\n",
            "  (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
            "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
            "  (fc1): Linear(in_features=9216, out_features=128, bias=False)\n",
            "  (fc2): Linear(in_features=128, out_features=10, bias=False)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KJMOZ1Qt69f3",
        "colab_type": "code",
        "outputId": "8e9741f4-d2e6-4d75-d808-045989328e6e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 271
        }
      },
      "source": [
        "wandb.init(project='debuggingnn')\n",
        "wandb.watch(net, log='all')\n",
        "\n",
        "for epoch in range(10):\n",
        "  train(net, device, trainloader, optimizer, epoch)\n",
        "  test(net, device, testloader, classes)\n",
        "\n",
        "print('Finished Training')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
              "                Project page: <a href=\"https://app.wandb.ai/ayush-thakur/debuggingnn\" target=\"_blank\">https://app.wandb.ai/ayush-thakur/debuggingnn</a><br/>\n",
              "                Run page: <a href=\"https://app.wandb.ai/ayush-thakur/debuggingnn/runs/5z09nhb9\" target=\"_blank\">https://app.wandb.ai/ayush-thakur/debuggingnn/runs/5z09nhb9</a><br/>\n",
              "            "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [0], Loss: 2.3370118141174316, Accuracy: 9.38,  Test_loss: 2.3214688762664797, Test_accuracy: 10.1\n",
            "Epoch [1], Loss: 2.302501916885376, Accuracy: 10.34,  Test_loss: 2.3092262718200685, Test_accuracy: 9.82\n",
            "Epoch [2], Loss: 2.306159734725952, Accuracy: 8.33,  Test_loss: 2.303833694076538, Test_accuracy: 10.1\n",
            "Epoch [3], Loss: 2.3103182315826416, Accuracy: 8.56,  Test_loss: 2.3044946189880373, Test_accuracy: 10.28\n",
            "Epoch [4], Loss: 2.3062429428100586, Accuracy: 11.01,  Test_loss: 2.30686653175354, Test_accuracy: 10.09\n",
            "Epoch [5], Loss: 2.318131923675537, Accuracy: 10.12,  Test_loss: 2.303500179672241, Test_accuracy: 11.35\n",
            "Epoch [6], Loss: 2.302870273590088, Accuracy: 9.97,  Test_loss: 2.307649087905884, Test_accuracy: 8.92\n",
            "Epoch [7], Loss: 2.3304078578948975, Accuracy: 10.12,  Test_loss: 2.30667571144104, Test_accuracy: 10.32\n",
            "Epoch [8], Loss: 2.3118438720703125, Accuracy: 9.23,  Test_loss: 2.3033378204345705, Test_accuracy: 11.35\n",
            "Epoch [9], Loss: 2.314885377883911, Accuracy: 9.6,  Test_loss: 2.3040501194000242, Test_accuracy: 9.74\n",
            "Finished Training\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yHHASFVtApW3",
        "colab_type": "text"
      },
      "source": [
        "> Our model couldn't converge. Check out the gradients of each layer on the W&B run page.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y534xBF1m70M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "del net"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Db6jPryFDa7m",
        "colab_type": "code",
        "outputId": "24ca51fb-6bbe-49ec-a99a-c91dcf84742e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 822
        }
      },
      "source": [
        "net = NetwithIssue().to(device)\n",
        "optimizer = optim.Adam(net.parameters())\n",
        "\n",
        "wandb.init(project='debuggingnn')\n",
        "wandb.watch(net, log='all')\n",
        "\n",
        "for epoch in range(40):\n",
        "  train(net, device, trainloader, optimizer, epoch)\n",
        "  test(net, device, testloader, classes)\n",
        "\n",
        "print('Finished Training')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
              "                Project page: <a href=\"https://app.wandb.ai/ayush-thakur/debuggingnn\" target=\"_blank\">https://app.wandb.ai/ayush-thakur/debuggingnn</a><br/>\n",
              "                Run page: <a href=\"https://app.wandb.ai/ayush-thakur/debuggingnn/runs/ldmjqpvm\" target=\"_blank\">https://app.wandb.ai/ayush-thakur/debuggingnn/runs/ldmjqpvm</a><br/>\n",
              "            "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [0], Loss: 2.3126368522644043, Accuracy: 8.93,  Test_loss: 2.3131574661254883, Test_accuracy: 9.8\n",
            "Epoch [1], Loss: 2.3287031650543213, Accuracy: 10.19,  Test_loss: 2.303910011672974, Test_accuracy: 10.28\n",
            "Epoch [2], Loss: 2.2527520656585693, Accuracy: 16.74,  Test_loss: 2.2425626552581788, Test_accuracy: 18.93\n",
            "Epoch [3], Loss: 2.14333438873291, Accuracy: 26.79,  Test_loss: 2.0964103660583495, Test_accuracy: 56.37\n",
            "Epoch [4], Loss: 1.8815544843673706, Accuracy: 52.38,  Test_loss: 1.913255361557007, Test_accuracy: 43.34\n",
            "Epoch [5], Loss: 1.662744402885437, Accuracy: 62.43,  Test_loss: 1.666070950126648, Test_accuracy: 68.16\n",
            "Epoch [6], Loss: 1.5230214595794678, Accuracy: 72.69,  Test_loss: 1.478899013710022, Test_accuracy: 78.22\n",
            "Epoch [7], Loss: 1.2913689613342285, Accuracy: 79.99,  Test_loss: 1.3357901279449462, Test_accuracy: 79.82\n",
            "Epoch [8], Loss: 1.2442892789840698, Accuracy: 80.06,  Test_loss: 1.1794730913162232, Test_accuracy: 83.65\n",
            "Epoch [9], Loss: 1.0750553607940674, Accuracy: 85.34,  Test_loss: 1.0552500459671021, Test_accuracy: 84.85\n",
            "Epoch [10], Loss: 1.0628418922424316, Accuracy: 85.19,  Test_loss: 0.9634736893653869, Test_accuracy: 85.58\n",
            "Epoch [11], Loss: 0.7314915657043457, Accuracy: 85.57,  Test_loss: 0.8507704361915588, Test_accuracy: 87.12\n",
            "Epoch [12], Loss: 0.7929632663726807, Accuracy: 86.83,  Test_loss: 0.7603620798110962, Test_accuracy: 88.69\n",
            "Epoch [13], Loss: 0.7627761363983154, Accuracy: 88.24,  Test_loss: 0.6911953255653381, Test_accuracy: 88.48\n",
            "Epoch [14], Loss: 0.6672923564910889, Accuracy: 85.71,  Test_loss: 0.6391272235870361, Test_accuracy: 89.55\n",
            "Epoch [15], Loss: 0.5932657122612, Accuracy: 88.84,  Test_loss: 0.5919654327392578, Test_accuracy: 90.45\n",
            "Epoch [16], Loss: 0.5837622284889221, Accuracy: 90.33,  Test_loss: 0.5397574633598328, Test_accuracy: 90.6\n",
            "Epoch [17], Loss: 0.5451374053955078, Accuracy: 89.81,  Test_loss: 0.5167940553665161, Test_accuracy: 90.83\n",
            "Epoch [18], Loss: 0.4775589108467102, Accuracy: 90.77,  Test_loss: 0.477168159198761, Test_accuracy: 91.68\n",
            "Epoch [19], Loss: 0.42984673380851746, Accuracy: 91.52,  Test_loss: 0.4535342445373535, Test_accuracy: 91.75\n",
            "Epoch [20], Loss: 0.5248159766197205, Accuracy: 91.29,  Test_loss: 0.4361233807563782, Test_accuracy: 91.61\n",
            "Epoch [21], Loss: 0.2923576235771179, Accuracy: 91.59,  Test_loss: 0.4051858386039734, Test_accuracy: 92.36\n",
            "Epoch [22], Loss: 0.35520315170288086, Accuracy: 91.22,  Test_loss: 0.3860480506896973, Test_accuracy: 92.49\n",
            "Epoch [23], Loss: 0.4020459055900574, Accuracy: 93.45,  Test_loss: 0.36066698427200317, Test_accuracy: 92.78\n",
            "Epoch [24], Loss: 0.37979257106781006, Accuracy: 92.34,  Test_loss: 0.338511720085144, Test_accuracy: 92.98\n",
            "Epoch [25], Loss: 0.29726701974868774, Accuracy: 93.15,  Test_loss: 0.33081806392669677, Test_accuracy: 93.05\n",
            "Epoch [26], Loss: 0.4136708080768585, Accuracy: 92.11,  Test_loss: 0.3275315603256226, Test_accuracy: 92.8\n",
            "Epoch [27], Loss: 0.3365878760814667, Accuracy: 93.6,  Test_loss: 0.3044555381536484, Test_accuracy: 93.48\n",
            "Epoch [28], Loss: 0.29751425981521606, Accuracy: 94.64,  Test_loss: 0.29189011104106904, Test_accuracy: 93.62\n",
            "Epoch [29], Loss: 0.32767513394355774, Accuracy: 92.19,  Test_loss: 0.29939857811927795, Test_accuracy: 93.53\n",
            "Epoch [30], Loss: 0.2649295926094055, Accuracy: 94.2,  Test_loss: 0.26654206233024597, Test_accuracy: 94.23\n",
            "Epoch [31], Loss: 0.24116864800453186, Accuracy: 93.53,  Test_loss: 0.26412147827148436, Test_accuracy: 94.11\n",
            "Epoch [32], Loss: 0.2848140299320221, Accuracy: 94.42,  Test_loss: 0.25166504876613616, Test_accuracy: 94.28\n",
            "Epoch [33], Loss: 0.3204647898674011, Accuracy: 92.86,  Test_loss: 0.24789080998897553, Test_accuracy: 94.4\n",
            "Epoch [34], Loss: 0.24367478489875793, Accuracy: 93.6,  Test_loss: 0.239266965174675, Test_accuracy: 94.59\n",
            "Epoch [35], Loss: 0.24248819053173065, Accuracy: 94.35,  Test_loss: 0.23756753034591674, Test_accuracy: 94.54\n",
            "Epoch [36], Loss: 0.27006995677948, Accuracy: 94.64,  Test_loss: 0.21805233216285705, Test_accuracy: 95.14\n",
            "Epoch [37], Loss: 0.12984707951545715, Accuracy: 95.46,  Test_loss: 0.2170099805355072, Test_accuracy: 94.75\n",
            "Epoch [38], Loss: 0.1676589548587799, Accuracy: 94.79,  Test_loss: 0.20750332999229432, Test_accuracy: 95.14\n",
            "Epoch [39], Loss: 0.29178619384765625, Accuracy: 95.16,  Test_loss: 0.2062499690055847, Test_accuracy: 94.95\n",
            "Finished Training\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ArPLSklZJqu4",
        "colab_type": "text"
      },
      "source": [
        "## Relu saves the day"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JCv0BZIUDd81",
        "colab_type": "code",
        "outputId": "d55dbe3a-e3bf-414e-a067-85df3e62def3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        }
      },
      "source": [
        "del net\n",
        "\n",
        "net = Net().to(device)\n",
        "print(net)\n",
        "\n",
        "optimizer = optim.Adam(net.parameters())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Net(\n",
            "  (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
            "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
            "  (fc1): Linear(in_features=9216, out_features=128, bias=False)\n",
            "  (fc2): Linear(in_features=128, out_features=10, bias=False)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jVGiu_9hJv9_",
        "colab_type": "code",
        "outputId": "ff6821ab-ab36-4ba7-81a2-e39fa864b069",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 271
        }
      },
      "source": [
        "wandb.init(project='debuggingnn')\n",
        "wandb.watch(net, log='all')\n",
        "\n",
        "for epoch in range(10):\n",
        "  train(net, device, trainloader, optimizer, epoch)\n",
        "  test(net, device, testloader, classes)\n",
        "\n",
        "print('Finished Training')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
              "                Project page: <a href=\"https://app.wandb.ai/ayush-thakur/debuggingnn\" target=\"_blank\">https://app.wandb.ai/ayush-thakur/debuggingnn</a><br/>\n",
              "                Run page: <a href=\"https://app.wandb.ai/ayush-thakur/debuggingnn/runs/q3zjcazj\" target=\"_blank\">https://app.wandb.ai/ayush-thakur/debuggingnn/runs/q3zjcazj</a><br/>\n",
              "            "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [0], Loss: 0.4942484498023987, Accuracy: 69.27,  Test_loss: 0.4474091837644577, Test_accuracy: 86.37\n",
            "Epoch [1], Loss: 0.29710474610328674, Accuracy: 88.47,  Test_loss: 0.2309967631816864, Test_accuracy: 93.38\n",
            "Epoch [2], Loss: 0.14050628244876862, Accuracy: 92.49,  Test_loss: 0.19611059620380403, Test_accuracy: 93.99\n",
            "Epoch [3], Loss: 0.11804260313510895, Accuracy: 94.94,  Test_loss: 0.1683878321647644, Test_accuracy: 94.79\n",
            "Epoch [4], Loss: 0.08817225694656372, Accuracy: 95.61,  Test_loss: 0.15559489104747773, Test_accuracy: 95.51\n",
            "Epoch [5], Loss: 0.035265251994132996, Accuracy: 95.68,  Test_loss: 0.1284967087507248, Test_accuracy: 96.07\n",
            "Epoch [6], Loss: 0.22864656150341034, Accuracy: 95.54,  Test_loss: 0.11613762941360474, Test_accuracy: 96.56\n",
            "Epoch [7], Loss: 0.136811763048172, Accuracy: 97.17,  Test_loss: 0.10356810202598572, Test_accuracy: 96.91\n",
            "Epoch [8], Loss: 0.1875208616256714, Accuracy: 96.95,  Test_loss: 0.11141893649101257, Test_accuracy: 96.51\n",
            "Epoch [9], Loss: 0.10076754540205002, Accuracy: 96.8,  Test_loss: 0.09019851398468018, Test_accuracy: 97.26\n",
            "Finished Training\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JgDAlzv7KSew",
        "colab_type": "text"
      },
      "source": [
        "> The model converged quickly. Through the gradient plot it can be shown clearly that the gradients are distributed well. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iLQm6Vjw4uwo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}