{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Cifar10_pytorch_wandb_Dropout_BN",
      "provenance": [],
      "authorship_tag": "ABX9TyPYYUJemdjJdpQU4/jSPe3p",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ayulockin/debugNNwithWandB/blob/master/Cifar10_pytorch_wandb_Dropout_BN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UJrTcams9S6G",
        "colab_type": "text"
      },
      "source": [
        "## Imports and Setups"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HESv1l--3AAa",
        "colab_type": "code",
        "outputId": "2c1423fb-6dbc-4f80-b462-29cc72d76575",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 256
        }
      },
      "source": [
        "!pip install wandb -q"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 1.4MB 3.5MB/s \n",
            "\u001b[K     |████████████████████████████████| 92kB 14.4MB/s \n",
            "\u001b[K     |████████████████████████████████| 102kB 13.8MB/s \n",
            "\u001b[K     |████████████████████████████████| 460kB 56.6MB/s \n",
            "\u001b[K     |████████████████████████████████| 102kB 15.1MB/s \n",
            "\u001b[K     |████████████████████████████████| 71kB 11.8MB/s \n",
            "\u001b[K     |████████████████████████████████| 71kB 9.1MB/s \n",
            "\u001b[?25h  Building wheel for watchdog (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for gql (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for shortuuid (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for subprocess32 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for graphql-core (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vg0iv_RPT3YG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import wandb"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4zcjcQ3-UH2H",
        "colab_type": "code",
        "outputId": "73bfbeab-873c-4897-cfdd-b2c5d02551c1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "source": [
        "!wandb login"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://app.wandb.ai/authorize\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter: 69f60a7711ce6b8bbae91ac6d15e45d6b1f1430e\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[32mSuccessfully logged in to Weights & Biases!\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_RFZ81yY3Rfl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "from torch.nn import functional as F\n",
        "import torchvision\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DtakKUgw9Ype",
        "colab_type": "text"
      },
      "source": [
        "#### For GPU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NiEHlMTh4ecC",
        "colab_type": "code",
        "outputId": "56634378-226f-4fe4-bf58-6c7a3ca2197a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TIxBcFVg9clX",
        "colab_type": "text"
      },
      "source": [
        "## CIFAR10 Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FtM4FMPP3imH",
        "colab_type": "code",
        "outputId": "b47f172d-ff81-4b0a-f104-1045e673b4a7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "source": [
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                        download=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=32,\n",
        "                                          shuffle=True)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                       download=True, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=32,\n",
        "                                         shuffle=False)\n",
        "\n",
        "classes = ('plane', 'car', 'bird', 'cat',\n",
        "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 0/170498071 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "170500096it [00:01, 90227723.22it/s]                               \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9E6tH8wrn2_Z",
        "colab_type": "code",
        "outputId": "0e975a14-4329-4ff0-b07a-f223c909a43e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 182
        }
      },
      "source": [
        "trainset"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset CIFAR10\n",
              "    Number of datapoints: 50000\n",
              "    Root location: ./data\n",
              "    Split: Train\n",
              "    StandardTransform\n",
              "Transform: Compose(\n",
              "               ToTensor()\n",
              "               Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n",
              "           )"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_8luRi8q9pyh",
        "colab_type": "text"
      },
      "source": [
        "## Base model\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z3rki8u__TU4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 32, 3, 1)\n",
        "        self.conv2 = nn.Conv2d(32, 32, 3, 1)\n",
        "        self.conv3 = nn.Conv2d(32, 64, 3, 1)\n",
        "        self.conv4 = nn.Conv2d(64, 64, 3, 1)\n",
        "\n",
        "        self.pool1 = torch.nn.MaxPool2d(2)\n",
        "        self.pool2 = torch.nn.MaxPool2d(2)\n",
        "\n",
        "        self.fc1 = nn.Linear(1600, 512)\n",
        "        self.fc2 = nn.Linear(512, 128)\n",
        "        self.fc3 = nn.Linear(128, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        ## Conv 1st Block\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = self.pool1(x)\n",
        "        x = F.relu(self.conv3(x))\n",
        "        x = F.relu(self.conv4(x))\n",
        "        x = self.pool2(x)\n",
        "        \n",
        "        x = torch.flatten(x, 1)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        output = F.log_softmax(x, dim=1)\n",
        "        return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z3Cv0XLX54Xo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(model, device, train_loader, optimizer, epoch, steps_per_epoch):\n",
        "  # Switch model to training mode. This is necessary for layers like dropout, batchnorm etc which behave differently in training and evaluation mode\n",
        "  model.train()\n",
        "  train_total = 0\n",
        "  train_correct = 0\n",
        "\n",
        "  # We loop over the data iterator, and feed the inputs to the network and adjust the weights.\n",
        "  for batch_idx, (data, target) in enumerate(train_loader, start=0):\n",
        "    if batch_idx > steps_per_epoch:\n",
        "      break\n",
        "    # Load the input features and labels from the training dataset\n",
        "    data, target = data.to(device), target.to(device)\n",
        "    \n",
        "    # Reset the gradients to 0 for all learnable weight parameters\n",
        "    optimizer.zero_grad()\n",
        "    \n",
        "    # Forward pass: Pass image data from training dataset, make predictions about class image belongs to (0-9 in this case)\n",
        "    output = model(data)\n",
        "    \n",
        "    # Define our loss function, and compute the loss\n",
        "    loss = F.nll_loss(output, target)\n",
        "\n",
        "    scores, predictions = torch.max(output.data, 1)\n",
        "    train_total += target.size(0)\n",
        "    train_correct += int(sum(predictions == target))\n",
        "            \n",
        "    # Backward pass: compute the gradients of the loss w.r.t. the model's parameters\n",
        "    loss.backward()\n",
        "    \n",
        "    # Update the neural network weights\n",
        "    optimizer.step()\n",
        "\n",
        "  acc = round((train_correct / train_total) * 100, 2)\n",
        "  print('Epoch [{}], Loss: {}, Accuracy: {}, '.format(epoch, loss.item(), acc), end='')\n",
        "  wandb.log({'Train Loss': loss.item(), 'Train Accuracy': acc})\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vHcCrrPJ65p-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test(model, device, test_loader, classes):\n",
        "  # Switch model to evaluation mode. This is necessary for layers like dropout, batchnorm etc which behave differently in training and evaluation mode\n",
        "  model.eval()\n",
        "  \n",
        "  test_loss = 0\n",
        "  test_total = 0\n",
        "  test_correct = 0\n",
        "\n",
        "  with torch.no_grad():\n",
        "      for data, target in test_loader:\n",
        "          # Load the input features and labels from the test dataset\n",
        "          data, target = data.to(device), target.to(device)\n",
        "          \n",
        "          # Make predictions: Pass image data from test dataset, make predictions about class image belongs to (0-9 in this case)\n",
        "          output = model(data)\n",
        "          \n",
        "          # Compute the loss sum up batch loss\n",
        "          test_loss += F.nll_loss(output, target, reduction='sum').item()\n",
        "          \n",
        "          scores, predictions = torch.max(output.data, 1)\n",
        "          test_total += target.size(0)\n",
        "          test_correct += int(sum(predictions == target))\n",
        "          \n",
        "  acc = round((test_correct / test_total) * 100, 2)\n",
        "  print(' Test_loss: {}, Test_accuracy: {}'.format(test_loss/test_total, acc))\n",
        "  wandb.log({'Test Loss': test_loss/test_total, 'Test Accuracy': acc})\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lZvNwsrk_xFa",
        "colab_type": "text"
      },
      "source": [
        "## Let's train it"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KYSE0Psw38w0",
        "colab_type": "code",
        "outputId": "5a5c3686-d5d2-437d-a5d0-fc37fc0aa1aa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        }
      },
      "source": [
        "net = Net().to(device)\n",
        "print(net)\n",
        "\n",
        "optimizer = optim.Adam(net.parameters())"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Net(\n",
            "  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1))\n",
            "  (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1))\n",
            "  (conv3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
            "  (conv4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
            "  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (fc1): Linear(in_features=1600, out_features=512, bias=True)\n",
            "  (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
            "  (fc3): Linear(in_features=128, out_features=10, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KJMOZ1Qt69f3",
        "colab_type": "code",
        "outputId": "69e68b32-36b2-4d26-ce3f-122ef00f9252",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 271
        }
      },
      "source": [
        "wandb.init(project='dropoutbn')\n",
        "wandb.watch(net, log='all')\n",
        "\n",
        "for epoch in range(10):\n",
        "  train(net, device, trainloader, optimizer, epoch, steps_per_epoch=50000//32)\n",
        "  test(net, device, testloader, classes)\n",
        "\n",
        "print('Finished Training')"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
              "                Project page: <a href=\"https://app.wandb.ai/ayush-thakur/dropoutbn\" target=\"_blank\">https://app.wandb.ai/ayush-thakur/dropoutbn</a><br/>\n",
              "                Run page: <a href=\"https://app.wandb.ai/ayush-thakur/dropoutbn/runs/dxdz06vk\" target=\"_blank\">https://app.wandb.ai/ayush-thakur/dropoutbn/runs/dxdz06vk</a><br/>\n",
              "            "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [0], Loss: 0.762065589427948, Accuracy: 45.91,  Test_loss: 1.1432553052902221, Test_accuracy: 59.17\n",
            "Epoch [1], Loss: 1.5598825216293335, Accuracy: 65.06,  Test_loss: 0.9075411169052124, Test_accuracy: 67.82\n",
            "Epoch [2], Loss: 0.5700869560241699, Accuracy: 72.46,  Test_loss: 0.8052263621330261, Test_accuracy: 72.72\n",
            "Epoch [3], Loss: 0.5809231400489807, Accuracy: 77.45,  Test_loss: 0.7728524807929993, Test_accuracy: 73.53\n",
            "Epoch [4], Loss: 0.6602230072021484, Accuracy: 81.08,  Test_loss: 0.8378544267654419, Test_accuracy: 72.13\n",
            "Epoch [5], Loss: 0.6705617904663086, Accuracy: 84.43,  Test_loss: 0.8033297297954559, Test_accuracy: 74.79\n",
            "Epoch [6], Loss: 0.10596936196088791, Accuracy: 87.31,  Test_loss: 0.9438407429695129, Test_accuracy: 73.73\n",
            "Epoch [7], Loss: 0.17943860590457916, Accuracy: 89.68,  Test_loss: 0.9510077392578125, Test_accuracy: 73.43\n",
            "Epoch [8], Loss: 0.17485202848911285, Accuracy: 91.63,  Test_loss: 1.0446744371414185, Test_accuracy: 74.01\n",
            "Epoch [9], Loss: 0.03261728584766388, Accuracy: 93.08,  Test_loss: 1.1755796172857285, Test_accuracy: 73.41\n",
            "Finished Training\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yHHASFVtApW3",
        "colab_type": "text"
      },
      "source": [
        "## Dropout\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZiiFdwKdoCOQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 32, 3, 1)\n",
        "        self.conv2 = nn.Conv2d(32, 32, 3, 1)\n",
        "        self.conv3 = nn.Conv2d(32, 64, 3, 1)\n",
        "        self.conv4 = nn.Conv2d(64, 64, 3, 1)\n",
        "\n",
        "        self.pool1 = torch.nn.MaxPool2d(2)\n",
        "        self.pool2 = torch.nn.MaxPool2d(2)\n",
        "\n",
        "        self.drop1 = torch.nn.Dropout()\n",
        "        self.drop2 = torch.nn.Dropout()\n",
        "\n",
        "        self.fc1 = nn.Linear(1600, 512)\n",
        "        self.fc2 = nn.Linear(512, 128)\n",
        "        self.fc3 = nn.Linear(128, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        ## Conv 1st Block\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = self.drop1(x)\n",
        "        x = self.pool1(x)\n",
        "        x = F.relu(self.conv3(x))\n",
        "        x = F.relu(self.conv4(x))\n",
        "        x = self.drop1(x)\n",
        "        x = self.pool2(x)\n",
        "        \n",
        "        x = torch.flatten(x, 1)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        output = F.log_softmax(x, dim=1)\n",
        "        return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y534xBF1m70M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "del net"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Db6jPryFDa7m",
        "colab_type": "code",
        "outputId": "8eb486e8-a4e4-4724-ae06-b2f2bd43faa4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 271
        }
      },
      "source": [
        "net = Net().to(device)\n",
        "optimizer = optim.Adam(net.parameters())\n",
        "\n",
        "wandb.init(project='dropoutbn')\n",
        "wandb.watch(net, log='all')\n",
        "\n",
        "for epoch in range(10):\n",
        "  train(net, device, trainloader, optimizer, epoch, steps_per_epoch=50000//32)\n",
        "  test(net, device, testloader, classes)\n",
        "\n",
        "print('Finished Training')"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
              "                Project page: <a href=\"https://app.wandb.ai/ayush-thakur/dropoutbn\" target=\"_blank\">https://app.wandb.ai/ayush-thakur/dropoutbn</a><br/>\n",
              "                Run page: <a href=\"https://app.wandb.ai/ayush-thakur/dropoutbn/runs/tlf4e8fm\" target=\"_blank\">https://app.wandb.ai/ayush-thakur/dropoutbn/runs/tlf4e8fm</a><br/>\n",
              "            "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [0], Loss: 1.3624159097671509, Accuracy: 46.28,  Test_loss: 1.4335350509643554, Test_accuracy: 55.48\n",
            "Epoch [1], Loss: 1.0600898265838623, Accuracy: 61.22,  Test_loss: 1.2644049947738647, Test_accuracy: 59.83\n",
            "Epoch [2], Loss: 0.7622907161712646, Accuracy: 66.91,  Test_loss: 1.1126571146011353, Test_accuracy: 63.32\n",
            "Epoch [3], Loss: 0.8105753064155579, Accuracy: 70.28,  Test_loss: 1.042283620452881, Test_accuracy: 66.07\n",
            "Epoch [4], Loss: 0.709924578666687, Accuracy: 73.27,  Test_loss: 1.0024064876556396, Test_accuracy: 66.34\n",
            "Epoch [5], Loss: 0.432053804397583, Accuracy: 75.23,  Test_loss: 0.9317248403549194, Test_accuracy: 69.53\n",
            "Epoch [6], Loss: 0.6623861789703369, Accuracy: 77.09,  Test_loss: 0.8552054161071777, Test_accuracy: 71.3\n",
            "Epoch [7], Loss: 0.6431245803833008, Accuracy: 78.63,  Test_loss: 0.8708825876235962, Test_accuracy: 72.19\n",
            "Epoch [8], Loss: 0.502291202545166, Accuracy: 79.91,  Test_loss: 0.849602875995636, Test_accuracy: 71.89\n",
            "Epoch [9], Loss: 1.0329746007919312, Accuracy: 81.03,  Test_loss: 0.8113357447624207, Test_accuracy: 72.73\n",
            "Finished Training\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qkz-DkLX3c59",
        "colab_type": "text"
      },
      "source": [
        "Note:\n",
        "\n",
        "1. The model didn't overfit. \n",
        "2. It took longer to converge. This can be because ensemble learning take time. Not every neuron was available while learning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CW0H45TW3Wrv",
        "colab_type": "text"
      },
      "source": [
        "## Batch Normalization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uZWWEjfgl_g6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 32, 3, 1)\n",
        "        self.conv2 = nn.Conv2d(32, 32, 3, 1)\n",
        "        self.conv3 = nn.Conv2d(32, 64, 3, 1)\n",
        "        self.conv4 = nn.Conv2d(64, 64, 3, 1)\n",
        "\n",
        "        self.pool1 = torch.nn.MaxPool2d(2)\n",
        "        self.pool2 = torch.nn.MaxPool2d(2)\n",
        "\n",
        "        self.bn1 = torch.nn.BatchNorm2d(32)\n",
        "        self.bn2 = torch.nn.BatchNorm2d(64)\n",
        "\n",
        "        self.fc1 = nn.Linear(1600, 512)\n",
        "        self.fc2 = nn.Linear(512, 128)\n",
        "        self.fc3 = nn.Linear(128, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        ## Conv 1st Block\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = F.relu(x)\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = self.pool1(x)\n",
        "\n",
        "        x = self.conv3(x)\n",
        "        x = self.bn2(x)\n",
        "        x = F.relu(x)\n",
        "        x = F.relu(self.conv4(x))\n",
        "        x = self.pool2(x)\n",
        "        \n",
        "        x = torch.flatten(x, 1)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        output = F.log_softmax(x, dim=1)\n",
        "        return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "b_iiehij7Me7",
        "colab": {}
      },
      "source": [
        "del net"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "7d80b91a-538b-40c9-993c-145bceeb76b5",
        "id": "P5Hh8ZN47MfL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 271
        }
      },
      "source": [
        "net = Net().to(device)\n",
        "optimizer = optim.Adam(net.parameters())\n",
        "\n",
        "wandb.init(project='dropoutbn')\n",
        "wandb.watch(net, log='all')\n",
        "\n",
        "for epoch in range(10):\n",
        "  train(net, device, trainloader, optimizer, epoch, steps_per_epoch=50000//32)\n",
        "  test(net, device, testloader, classes)\n",
        "\n",
        "print('Finished Training')"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
              "                Project page: <a href=\"https://app.wandb.ai/ayush-thakur/dropoutbn\" target=\"_blank\">https://app.wandb.ai/ayush-thakur/dropoutbn</a><br/>\n",
              "                Run page: <a href=\"https://app.wandb.ai/ayush-thakur/dropoutbn/runs/pntlvpic\" target=\"_blank\">https://app.wandb.ai/ayush-thakur/dropoutbn/runs/pntlvpic</a><br/>\n",
              "            "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [0], Loss: 1.2230021953582764, Accuracy: 52.4,  Test_loss: 1.0370957496643067, Test_accuracy: 63.41\n",
            "Epoch [1], Loss: 0.9987068176269531, Accuracy: 69.23,  Test_loss: 0.8546534811973572, Test_accuracy: 70.51\n",
            "Epoch [2], Loss: 0.3769473433494568, Accuracy: 75.29,  Test_loss: 0.7601750534057617, Test_accuracy: 74.39\n",
            "Epoch [3], Loss: 0.4631640613079071, Accuracy: 79.41,  Test_loss: 0.748601777267456, Test_accuracy: 75.26\n",
            "Epoch [4], Loss: 0.6603724956512451, Accuracy: 82.82,  Test_loss: 0.8692990935325623, Test_accuracy: 72.67\n",
            "Epoch [5], Loss: 0.08839832246303558, Accuracy: 85.81,  Test_loss: 0.75582248544693, Test_accuracy: 76.97\n",
            "Epoch [6], Loss: 0.1988542228937149, Accuracy: 88.21,  Test_loss: 0.8076319061279297, Test_accuracy: 75.69\n",
            "Epoch [7], Loss: 0.42038971185684204, Accuracy: 90.4,  Test_loss: 0.9096485228061676, Test_accuracy: 75.28\n",
            "Epoch [8], Loss: 0.4379485845565796, Accuracy: 91.9,  Test_loss: 1.0178819646835326, Test_accuracy: 75.0\n",
            "Epoch [9], Loss: 0.8714055418968201, Accuracy: 93.09,  Test_loss: 1.0234106095790863, Test_accuracy: 75.24\n",
            "Finished Training\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y8TNuAX381mF",
        "colab_type": "text"
      },
      "source": [
        "Note:\n",
        "1. The model converges quickly.\n",
        "2. Because the model is still simple for the standard of BN, overfitting occured. \n",
        "\n",
        "Let's see Batch Normalization and Dropout in action together."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TYRstLCV9kk7",
        "colab_type": "text"
      },
      "source": [
        "## Batch Normalization and Dropout"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lXmpyesG9hDi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 32, 3, 1)\n",
        "        self.conv2 = nn.Conv2d(32, 32, 3, 1)\n",
        "        self.conv3 = nn.Conv2d(32, 64, 3, 1)\n",
        "        self.conv4 = nn.Conv2d(64, 64, 3, 1)\n",
        "\n",
        "        self.pool1 = torch.nn.MaxPool2d(2)\n",
        "        self.pool2 = torch.nn.MaxPool2d(2)\n",
        "\n",
        "        self.bn1 = torch.nn.BatchNorm2d(32)\n",
        "        self.bn2 = torch.nn.BatchNorm2d(64)\n",
        "\n",
        "        self.drop1 = torch.nn.Dropout()\n",
        "        self.drop2 = torch.nn.Dropout()\n",
        "\n",
        "        self.fc1 = nn.Linear(1600, 512)\n",
        "        self.fc2 = nn.Linear(512, 128)\n",
        "        self.fc3 = nn.Linear(128, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        ## Conv 1st Block\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = F.relu(x)\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = self.drop1(x)\n",
        "        x = self.pool1(x)\n",
        "\n",
        "        x = self.conv3(x)\n",
        "        x = self.bn2(x)\n",
        "        x = F.relu(x)\n",
        "        x = F.relu(self.conv4(x))\n",
        "        x = self.drop2(x)\n",
        "        x = self.pool2(x)\n",
        "        \n",
        "        x = torch.flatten(x, 1)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        output = F.log_softmax(x, dim=1)\n",
        "        return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "50hp_3Ig-dvA",
        "colab": {}
      },
      "source": [
        "del net"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "216c2521-6823-4275-9d36-41e0afe3c667",
        "id": "QYyKoX0K-dvV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 271
        }
      },
      "source": [
        "net = Net().to(device)\n",
        "optimizer = optim.Adam(net.parameters())\n",
        "\n",
        "wandb.init(project='dropoutbn')\n",
        "wandb.watch(net, log='all')\n",
        "\n",
        "for epoch in range(10):\n",
        "  train(net, device, trainloader, optimizer, epoch, steps_per_epoch=50000//32)\n",
        "  test(net, device, testloader, classes)\n",
        "\n",
        "print('Finished Training')"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
              "                Project page: <a href=\"https://app.wandb.ai/ayush-thakur/dropoutbn\" target=\"_blank\">https://app.wandb.ai/ayush-thakur/dropoutbn</a><br/>\n",
              "                Run page: <a href=\"https://app.wandb.ai/ayush-thakur/dropoutbn/runs/1e5c7bk2\" target=\"_blank\">https://app.wandb.ai/ayush-thakur/dropoutbn/runs/1e5c7bk2</a><br/>\n",
              "            "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [0], Loss: 1.483834981918335, Accuracy: 50.59,  Test_loss: 1.3474351978302002, Test_accuracy: 55.21\n",
            "Epoch [1], Loss: 1.0609911680221558, Accuracy: 64.46,  Test_loss: 1.1758262783050537, Test_accuracy: 61.54\n",
            "Epoch [2], Loss: 0.6946431398391724, Accuracy: 70.01,  Test_loss: 0.9932758785247803, Test_accuracy: 67.75\n",
            "Epoch [3], Loss: 0.5579437017440796, Accuracy: 73.12,  Test_loss: 0.9274280029296875, Test_accuracy: 70.64\n",
            "Epoch [4], Loss: 0.40007323026657104, Accuracy: 75.78,  Test_loss: 0.8967650428771973, Test_accuracy: 71.24\n",
            "Epoch [5], Loss: 0.22875693440437317, Accuracy: 77.66,  Test_loss: 0.9264094530105591, Test_accuracy: 69.76\n",
            "Epoch [6], Loss: 0.4890390634536743, Accuracy: 79.2,  Test_loss: 0.8634264698028564, Test_accuracy: 71.48\n",
            "Epoch [7], Loss: 0.571390688419342, Accuracy: 80.38,  Test_loss: 0.8696962162017823, Test_accuracy: 70.65\n",
            "Epoch [8], Loss: 0.2613355219364166, Accuracy: 81.8,  Test_loss: 0.8504735969543457, Test_accuracy: 70.83\n",
            "Epoch [9], Loss: 0.28765809535980225, Accuracy: 82.94,  Test_loss: 0.8524785316467285, Test_accuracy: 70.75\n",
            "Finished Training\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zw2EtXpjF5tI",
        "colab_type": "text"
      },
      "source": [
        "Note:\n",
        "\n",
        "1. Overfitting was avoided. \n",
        "2. The model converged better as compared to Dropout layer. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-2RLCMdH7JrJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}