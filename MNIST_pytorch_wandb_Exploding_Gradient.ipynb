{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MNIST_pytorch_wandb_Exploding_Gradient",
      "provenance": [],
      "authorship_tag": "ABX9TyNv2iWrXRBmfig5X54jb2Ef",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ayulockin/debugNNwithWandB/blob/master/MNIST_pytorch_wandb_Exploding_Gradient.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UJrTcams9S6G",
        "colab_type": "text"
      },
      "source": [
        "## Imports and Setups"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HESv1l--3AAa",
        "colab_type": "code",
        "outputId": "9c83bc0e-3be5-45ad-cfc7-01d07ce77ee4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 256
        }
      },
      "source": [
        "!pip install wandb -q"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 1.4MB 8.9MB/s \n",
            "\u001b[K     |████████████████████████████████| 92kB 9.9MB/s \n",
            "\u001b[K     |████████████████████████████████| 102kB 9.0MB/s \n",
            "\u001b[K     |████████████████████████████████| 460kB 27.1MB/s \n",
            "\u001b[K     |████████████████████████████████| 102kB 14.4MB/s \n",
            "\u001b[K     |████████████████████████████████| 71kB 11.1MB/s \n",
            "\u001b[K     |████████████████████████████████| 71kB 11.3MB/s \n",
            "\u001b[?25h  Building wheel for gql (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for watchdog (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for shortuuid (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for subprocess32 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for graphql-core (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vg0iv_RPT3YG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import wandb"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4zcjcQ3-UH2H",
        "colab_type": "code",
        "outputId": "d90b0be0-8f58-4134-8ee0-1e41ed71a5ad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "source": [
        "!wandb login"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://app.wandb.ai/authorize\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter: 69f60a7711ce6b8bbae91ac6d15e45d6b1f1430e\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[32mSuccessfully logged in to Weights & Biases!\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_RFZ81yY3Rfl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "from torch.nn import functional as F\n",
        "import torchvision\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DtakKUgw9Ype",
        "colab_type": "text"
      },
      "source": [
        "#### For GPU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NiEHlMTh4ecC",
        "colab_type": "code",
        "outputId": "02e2c192-5ece-409c-d1f4-330e3b602e17",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TIxBcFVg9clX",
        "colab_type": "text"
      },
      "source": [
        "## MNIST Hand written Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FtM4FMPP3imH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 274
        },
        "outputId": "21840da4-5152-4dc8-8959-2b30912ac576"
      },
      "source": [
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize((0.1307,), (0.3081,))])\n",
        "\n",
        "trainset = torchvision.datasets.MNIST(root='./data', train=True,\n",
        "                                        download=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
        "\n",
        "testset = torchvision.datasets.MNIST(root='./data', train=False,\n",
        "                                       download=True, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=False)\n",
        "\n",
        "classes = ('0', '1', '2', '3', '4', '5', '6', '7', '8', '9')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "9920512it [00:02, 3372615.73it/s]                            \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "32768it [00:00, 48231.78it/s]                           \n",
            "0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "1654784it [00:02, 816838.33it/s]                             \n",
            "0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "8192it [00:00, 18231.71it/s]            "
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "Processing...\n",
            "Done!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_8luRi8q9pyh",
        "colab_type": "text"
      },
      "source": [
        "## Models\n",
        "\n",
        "1.   `NetwithIssue`: This model uses `sigmoid` as activation function in each layer. Earlier when `relu` was not discovered `sigmoid` created the problem of **Vanishing Gradient**. Due to this deep networks were not possible and it took longer training time to converge. \n",
        "2.   `Net`: This model uses `relu` as activation function in each layer. This activation solves the problem of vanishing gradient.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CUOU5phlFoZn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class NetforExplode(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(NetforExplode, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
        "        self.conv1.weight.data.fill_(100)\n",
        "        self.conv1.bias.data.fill_(-100)\n",
        "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
        "        self.conv2.weight.data.fill_(100)\n",
        "        self.conv2.bias.data.fill_(-100)\n",
        "\n",
        "        self.fc1 = nn.Linear(9216, 128)\n",
        "        self.fc1.weight.data.fill_(100)\n",
        "        self.fc1.bias.data.fill_(-100)\n",
        "        self.fc2 = nn.Linear(128, 10)\n",
        "        self.fc2.weight.data.fill_(100)\n",
        "        self.fc2.bias.data.fill_(-100)\n",
        "        \n",
        "\n",
        "    def forward(self, x):\n",
        "        ## Conv 1st Block\n",
        "        x = self.conv1(x)\n",
        "        x = F.relu(x) ## Notice\n",
        "        x = self.conv2(x)\n",
        "        x = F.relu(x) ## Notice\n",
        "        x = F.max_pool2d(x, 2)\n",
        "\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.fc1(x)\n",
        "        x = F.relu(x) ## Notice\n",
        "        x = self.fc2(x)\n",
        "        output = F.log_softmax(x, dim=1)\n",
        "        return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z3rki8u__TU4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 32, 3, 1, bias=False)\n",
        "        self.conv2 = nn.Conv2d(32, 64, 3, 1, bias=False)\n",
        "\n",
        "        self.fc1 = nn.Linear(9216, 128, bias=False)\n",
        "        self.fc2 = nn.Linear(128, 10, bias=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        ## Conv 1st Block\n",
        "        x = self.conv1(x)\n",
        "        x = F.relu(x) ## Notice\n",
        "        x = self.conv2(x)\n",
        "        x = F.relu(x) ## Notice\n",
        "        x = F.max_pool2d(x, 2)\n",
        "\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.fc1(x)\n",
        "        x = F.relu(x) ## Notice\n",
        "        x = self.fc2(x)\n",
        "        output = F.log_softmax(x, dim=1)\n",
        "        return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z3Cv0XLX54Xo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(model, device, train_loader, optimizer, epoch, steps_per_epoch=20):\n",
        "  # Switch model to training mode. This is necessary for layers like dropout, batchnorm etc which behave differently in training and evaluation mode\n",
        "  model.train()\n",
        "  train_total = 0\n",
        "  train_correct = 0\n",
        "\n",
        "  # We loop over the data iterator, and feed the inputs to the network and adjust the weights.\n",
        "  for batch_idx, (data, target) in enumerate(train_loader, start=0):\n",
        "    if batch_idx > steps_per_epoch:\n",
        "      break\n",
        "    # Load the input features and labels from the training dataset\n",
        "    data, target = data.to(device), target.to(device)\n",
        "    \n",
        "    # Reset the gradients to 0 for all learnable weight parameters\n",
        "    optimizer.zero_grad()\n",
        "    \n",
        "    # Forward pass: Pass image data from training dataset, make predictions about class image belongs to (0-9 in this case)\n",
        "    output = model(data)\n",
        "    \n",
        "    # Define our loss function, and compute the loss\n",
        "    loss = F.nll_loss(output, target)\n",
        "\n",
        "    scores, predictions = torch.max(output.data, 1)\n",
        "    train_total += target.size(0)\n",
        "    train_correct += int(sum(predictions == target))\n",
        "            \n",
        "    # Backward pass: compute the gradients of the loss w.r.t. the model's parameters\n",
        "    loss.backward()\n",
        "    \n",
        "    # Update the neural network weights\n",
        "    optimizer.step()\n",
        "\n",
        "  acc = round((train_correct / train_total) * 100, 2)\n",
        "  print('Epoch [{}], Loss: {}, Accuracy: {}, '.format(epoch, loss.item(), acc), end='')\n",
        "  # wandb.log({'Train Loss': loss.item(), 'Train Accuracy': acc})\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vHcCrrPJ65p-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test(model, device, test_loader, classes):\n",
        "  # Switch model to evaluation mode. This is necessary for layers like dropout, batchnorm etc which behave differently in training and evaluation mode\n",
        "  model.eval()\n",
        "  \n",
        "  test_loss = 0\n",
        "  test_total = 0\n",
        "  test_correct = 0\n",
        "\n",
        "  with torch.no_grad():\n",
        "      for data, target in test_loader:\n",
        "          # Load the input features and labels from the test dataset\n",
        "          data, target = data.to(device), target.to(device)\n",
        "          \n",
        "          # Make predictions: Pass image data from test dataset, make predictions about class image belongs to (0-9 in this case)\n",
        "          output = model(data)\n",
        "          \n",
        "          # Compute the loss sum up batch loss\n",
        "          test_loss += F.nll_loss(output, target, reduction='sum').item()\n",
        "          \n",
        "          scores, predictions = torch.max(output.data, 1)\n",
        "          test_total += target.size(0)\n",
        "          test_correct += int(sum(predictions == target))\n",
        "          \n",
        "  acc = round((test_correct / test_total) * 100, 2)\n",
        "  print(' Test_loss: {}, Test_accuracy: {}'.format(test_loss/test_total, acc))\n",
        "  # wandb.log({'Test Loss': test_loss/test_total, 'Test Accuracy': acc})\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lZvNwsrk_xFa",
        "colab_type": "text"
      },
      "source": [
        "## Exploding Gradient (Network with Issue)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KYSE0Psw38w0",
        "colab_type": "code",
        "outputId": "7f96cb3a-672b-403f-bafd-7a560a7a2c96",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        }
      },
      "source": [
        "net = NetforExplode().to(device)\n",
        "print(net)\n",
        "\n",
        "optimizer = optim.Adam(net.parameters())"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "NetforExplode(\n",
            "  (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1))\n",
            "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
            "  (fc1): Linear(in_features=9216, out_features=128, bias=True)\n",
            "  (fc2): Linear(in_features=128, out_features=10, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KJMOZ1Qt69f3",
        "colab_type": "code",
        "outputId": "e7757f23-823c-47be-dc89-cd0092a20956",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        }
      },
      "source": [
        "# wandb.init(project='explodingdebug')\n",
        "# wandb.watch(net, log='all')\n",
        "\n",
        "for epoch in range(10):\n",
        "  train(net, device, trainloader, optimizer, epoch)\n",
        "  test(net, device, testloader, classes)\n",
        "\n",
        "print('Finished Training')"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch [0], Loss: 374601678848.0, Accuracy: 9.15,  Test_loss: 515360212543.0784, Test_accuracy: 10.28\n",
            "Epoch [1], Loss: 651828396032.0, Accuracy: 8.85,  Test_loss: 683000517047.0912, Test_accuracy: 10.1\n",
            "Epoch [2], Loss: 518415974400.0, Accuracy: 9.97,  Test_loss: 573574628769.792, Test_accuracy: 11.35\n",
            "Epoch [3], Loss: 557607550976.0, Accuracy: 9.23,  Test_loss: 706371366839.9104, Test_accuracy: 9.82\n",
            "Epoch [4], Loss: 817587290112.0, Accuracy: 11.09,  Test_loss: 799592343509.4016, Test_accuracy: 9.58\n",
            "Epoch [5], Loss: 762960674816.0, Accuracy: 10.27,  Test_loss: 942930434562.4576, Test_accuracy: 9.58\n",
            "Epoch [6], Loss: 665451495424.0, Accuracy: 9.75,  Test_loss: 655409432389.2224, Test_accuracy: 9.29\n",
            "Epoch [7], Loss: 877649723392.0, Accuracy: 10.49,  Test_loss: 1016483252745.0112, Test_accuracy: 9.8\n",
            "Epoch [8], Loss: 550259130368.0, Accuracy: 11.16,  Test_loss: 430533319956.8896, Test_accuracy: 10.09\n",
            "Epoch [9], Loss: 626729680896.0, Accuracy: 8.48,  Test_loss: 740067318012.3136, Test_accuracy: 10.32\n",
            "Finished Training\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yHHASFVtApW3",
        "colab_type": "text"
      },
      "source": [
        "> Our model couldn't converge. Let's look at the gradients of each layer. \n",
        "\n",
        "![gradients](link to github/images)\n",
        "\n",
        "Let's try to drecrease loss and increase accuracy by training it for longer.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y534xBF1m70M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "del net"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Db6jPryFDa7m",
        "colab_type": "code",
        "outputId": "496b42d2-4d62-40b2-ced6-1c4d95f93b04",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        }
      },
      "source": [
        "net = NetforExplode().to(device)\n",
        "optimizer = optim.Adam(net.parameters())\n",
        "\n",
        "# wandb.init(project='debuggingnn')\n",
        "# wandb.watch(net, log='all')\n",
        "\n",
        "for epoch in range(10):\n",
        "  train(net, device, trainloader, optimizer, epoch)\n",
        "  test(net, device, testloader, classes)\n",
        "\n",
        "print('Finished Training')"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch [0], Loss: 736184238080.0, Accuracy: 10.71,  Test_loss: 685160885596.9792, Test_accuracy: 9.74\n",
            "Epoch [1], Loss: 383292276736.0, Accuracy: 9.45,  Test_loss: 579661026924.9536, Test_accuracy: 10.09\n",
            "Epoch [2], Loss: 843826855936.0, Accuracy: 9.9,  Test_loss: 1530725270552.576, Test_accuracy: 10.1\n",
            "Epoch [3], Loss: 705851031552.0, Accuracy: 9.9,  Test_loss: 784419297794.4576, Test_accuracy: 10.32\n",
            "Epoch [4], Loss: 1398146072576.0, Accuracy: 10.04,  Test_loss: 1100310706441.421, Test_accuracy: 8.92\n",
            "Epoch [5], Loss: 631964172288.0, Accuracy: 10.42,  Test_loss: 675881608753.9712, Test_accuracy: 9.8\n",
            "Epoch [6], Loss: 418658648064.0, Accuracy: 10.19,  Test_loss: 499995826035.0976, Test_accuracy: 9.98\n",
            "Epoch [7], Loss: 1115215101952.0, Accuracy: 10.49,  Test_loss: 795683708521.6768, Test_accuracy: 11.35\n",
            "Epoch [8], Loss: 1363853443072.0, Accuracy: 10.79,  Test_loss: 1383919422904.7295, Test_accuracy: 8.92\n",
            "Epoch [9], Loss: 500363689984.0, Accuracy: 10.42,  Test_loss: 627344827586.9696, Test_accuracy: 10.09\n",
            "Finished Training\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ArPLSklZJqu4",
        "colab_type": "text"
      },
      "source": [
        "## Relu saves the day"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JCv0BZIUDd81",
        "colab_type": "code",
        "outputId": "c41fb79f-4f46-4cdc-bdfd-b35fd55681d5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        }
      },
      "source": [
        "del net\n",
        "\n",
        "net = Net().to(device)\n",
        "print(net)\n",
        "\n",
        "optimizer = optim.Adam(net.parameters())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Net(\n",
            "  (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
            "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
            "  (fc1): Linear(in_features=9216, out_features=128, bias=False)\n",
            "  (fc2): Linear(in_features=128, out_features=10, bias=False)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jVGiu_9hJv9_",
        "colab_type": "code",
        "outputId": "ff6821ab-ab36-4ba7-81a2-e39fa864b069",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 271
        }
      },
      "source": [
        "wandb.init(project='debuggingnn')\n",
        "wandb.watch(net, log='all')\n",
        "\n",
        "for epoch in range(10):\n",
        "  train(net, device, trainloader, optimizer, epoch)\n",
        "  test(net, device, testloader, classes)\n",
        "\n",
        "print('Finished Training')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
              "                Project page: <a href=\"https://app.wandb.ai/ayush-thakur/debuggingnn\" target=\"_blank\">https://app.wandb.ai/ayush-thakur/debuggingnn</a><br/>\n",
              "                Run page: <a href=\"https://app.wandb.ai/ayush-thakur/debuggingnn/runs/q3zjcazj\" target=\"_blank\">https://app.wandb.ai/ayush-thakur/debuggingnn/runs/q3zjcazj</a><br/>\n",
              "            "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [0], Loss: 0.4942484498023987, Accuracy: 69.27,  Test_loss: 0.4474091837644577, Test_accuracy: 86.37\n",
            "Epoch [1], Loss: 0.29710474610328674, Accuracy: 88.47,  Test_loss: 0.2309967631816864, Test_accuracy: 93.38\n",
            "Epoch [2], Loss: 0.14050628244876862, Accuracy: 92.49,  Test_loss: 0.19611059620380403, Test_accuracy: 93.99\n",
            "Epoch [3], Loss: 0.11804260313510895, Accuracy: 94.94,  Test_loss: 0.1683878321647644, Test_accuracy: 94.79\n",
            "Epoch [4], Loss: 0.08817225694656372, Accuracy: 95.61,  Test_loss: 0.15559489104747773, Test_accuracy: 95.51\n",
            "Epoch [5], Loss: 0.035265251994132996, Accuracy: 95.68,  Test_loss: 0.1284967087507248, Test_accuracy: 96.07\n",
            "Epoch [6], Loss: 0.22864656150341034, Accuracy: 95.54,  Test_loss: 0.11613762941360474, Test_accuracy: 96.56\n",
            "Epoch [7], Loss: 0.136811763048172, Accuracy: 97.17,  Test_loss: 0.10356810202598572, Test_accuracy: 96.91\n",
            "Epoch [8], Loss: 0.1875208616256714, Accuracy: 96.95,  Test_loss: 0.11141893649101257, Test_accuracy: 96.51\n",
            "Epoch [9], Loss: 0.10076754540205002, Accuracy: 96.8,  Test_loss: 0.09019851398468018, Test_accuracy: 97.26\n",
            "Finished Training\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JgDAlzv7KSew",
        "colab_type": "text"
      },
      "source": [
        "> The model converged quickly. Through the gradient plot it can be shown clearly that the gradients are distributed well. \n",
        "\n",
        "[LINK to]"
      ]
    }
  ]
}